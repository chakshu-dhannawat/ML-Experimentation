{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BLOOM_Language Model_try.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP9dxdi/v7Yz0wUrV27BP0r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4252f2fe414e4ff0bda218009b885fab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9291e8138f2b4901bf9e85d9af6003e0",
              "IPY_MODEL_5b2eb261083b4273ac62614a998d38f9",
              "IPY_MODEL_4877e7de493f43f2880cdc91372bed01",
              "IPY_MODEL_182951243a5646c2a46248fc0ef9e0e4"
            ],
            "layout": "IPY_MODEL_ef8d0922ac49429580ba7640ab06f635"
          }
        },
        "9291e8138f2b4901bf9e85d9af6003e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a952b2456f184d01bb25d8fef90cd069",
            "placeholder": "​",
            "style": "IPY_MODEL_a735cf9d641349e2b42981fdc5fe7980",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "5b2eb261083b4273ac62614a998d38f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_afb2897ee7664a0c904e8298f7584f1b",
            "placeholder": "​",
            "style": "IPY_MODEL_80d7d53b5edb49a880b8d151bcead760",
            "value": ""
          }
        },
        "4877e7de493f43f2880cdc91372bed01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_a553bf3423aa4b2b8718992dc4b1ba05",
            "style": "IPY_MODEL_74d8d31afb9540b8a1130cd395b85c41",
            "tooltip": ""
          }
        },
        "182951243a5646c2a46248fc0ef9e0e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0e886a185ed40698202c84b020f42b0",
            "placeholder": "​",
            "style": "IPY_MODEL_8405f5d1afb0479caa002a3695235fd0",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "ef8d0922ac49429580ba7640ab06f635": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "a952b2456f184d01bb25d8fef90cd069": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a735cf9d641349e2b42981fdc5fe7980": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "afb2897ee7664a0c904e8298f7584f1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80d7d53b5edb49a880b8d151bcead760": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a553bf3423aa4b2b8718992dc4b1ba05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74d8d31afb9540b8a1130cd395b85c41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "c0e886a185ed40698202c84b020f42b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8405f5d1afb0479caa002a3695235fd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chakshu-dhannawat/ML-Experimentation/blob/main/BLOOM_Language_Model_try.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbLtim_y8NWZ"
      },
      "outputs": [],
      "source": [
        "#Reference:\n",
        "#https://huggingface.co/blog/bloom"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#In this file, I have experimented with the language model BLOOM, and made inferences through it"
      ],
      "metadata": {
        "id": "nALuVoae8h-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# requirements:\n",
        "! pip install huggingface_hub\n",
        "! git config --global credential.helper store"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5kQtTONgtmX",
        "outputId": "1bca3894-97d2-403e-fb48-3d1be14acd8b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.64.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 47.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.1.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.12.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface_hub) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface_hub) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (3.0.4)\n",
            "Installing collected packages: pyyaml, huggingface-hub\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "from huggingface_hub import HfFolder\n",
        "\n",
        "\n",
        "#enter your API key, you can make one for free on HF\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236,
          "referenced_widgets": [
            "4252f2fe414e4ff0bda218009b885fab",
            "9291e8138f2b4901bf9e85d9af6003e0",
            "5b2eb261083b4273ac62614a998d38f9",
            "4877e7de493f43f2880cdc91372bed01",
            "182951243a5646c2a46248fc0ef9e0e4",
            "ef8d0922ac49429580ba7640ab06f635",
            "a952b2456f184d01bb25d8fef90cd069",
            "a735cf9d641349e2b42981fdc5fe7980",
            "afb2897ee7664a0c904e8298f7584f1b",
            "80d7d53b5edb49a880b8d151bcead760",
            "a553bf3423aa4b2b8718992dc4b1ba05",
            "74d8d31afb9540b8a1130cd395b85c41",
            "c0e886a185ed40698202c84b020f42b0",
            "8405f5d1afb0479caa002a3695235fd0"
          ]
        },
        "id": "Ee-Z3fizwVfK",
        "outputId": "35684092-f299-431c-ca64-c6523f7a9219"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Login successful\n",
            "Your token has been saved to /root/.huggingface/token\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceApi\n",
        "\n",
        "inference = InferenceApi(\"bigscience/bloom\",token=HfFolder.get_token())"
      ],
      "metadata": {
        "id": "wb0Ku2-Gwgmj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(inference)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H2MkcTv1u2G",
        "outputId": "f7b967b3-3e75-4d20-89b2-359f5e160669"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on InferenceApi in module huggingface_hub.inference_api object:\n",
            "\n",
            "class InferenceApi(builtins.object)\n",
            " |  InferenceApi(repo_id: str, task: Union[str, NoneType] = None, token: Union[str, NoneType] = None, gpu: Union[bool, NoneType] = False)\n",
            " |  \n",
            " |  Client to configure requests and make calls to the HuggingFace Inference API.\n",
            " |  \n",
            " |  Example:\n",
            " |  \n",
            " |  ```python\n",
            " |  >>> from huggingface_hub.inference_api import InferenceApi\n",
            " |  \n",
            " |  >>> # Mask-fill example\n",
            " |  >>> inference = InferenceApi(\"bert-base-uncased\")\n",
            " |  >>> inference(inputs=\"The goal of life is [MASK].\")\n",
            " |  [{'sequence': 'the goal of life is life.', 'score': 0.10933292657136917, 'token': 2166, 'token_str': 'life'}]\n",
            " |  \n",
            " |  >>> # Question Answering example\n",
            " |  >>> inference = InferenceApi(\"deepset/roberta-base-squad2\")\n",
            " |  >>> inputs = {\n",
            " |  ...     \"question\": \"What's my name?\",\n",
            " |  ...     \"context\": \"My name is Clara and I live in Berkeley.\",\n",
            " |  ... }\n",
            " |  >>> inference(inputs)\n",
            " |  {'score': 0.9326569437980652, 'start': 11, 'end': 16, 'answer': 'Clara'}\n",
            " |  \n",
            " |  >>> # Zero-shot example\n",
            " |  >>> inference = InferenceApi(\"typeform/distilbert-base-uncased-mnli\")\n",
            " |  >>> inputs = \"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!\"\n",
            " |  >>> params = {\"candidate_labels\": [\"refund\", \"legal\", \"faq\"]}\n",
            " |  >>> inference(inputs, params)\n",
            " |  {'sequence': 'Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!', 'labels': ['refund', 'faq', 'legal'], 'scores': [0.9378499388694763, 0.04914155602455139, 0.013008488342165947]}\n",
            " |  \n",
            " |  >>> # Overriding configured task\n",
            " |  >>> inference = InferenceApi(\"bert-base-uncased\", task=\"feature-extraction\")\n",
            " |  ```\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __call__(self, inputs: Union[str, Dict, List[str], List[List[str]], NoneType] = None, params: Union[Dict, NoneType] = None, data: Union[bytes, NoneType] = None)\n",
            " |      Call self as a function.\n",
            " |  \n",
            " |  __init__(self, repo_id: str, task: Union[str, NoneType] = None, token: Union[str, NoneType] = None, gpu: Union[bool, NoneType] = False)\n",
            " |      Inits headers and API call information.\n",
            " |      \n",
            " |      Args:\n",
            " |          repo_id (``str``):\n",
            " |              Id of repository (e.g. `user/bert-base-uncased`).\n",
            " |          task (``str``, `optional`, defaults ``None``):\n",
            " |              Whether to force a task instead of using task specified in the\n",
            " |              repository.\n",
            " |          token (`str`, `optional`):\n",
            " |              The API token to use as HTTP bearer authorization. This is not\n",
            " |              the authentication token. You can find the token in\n",
            " |              https://huggingface.co/settings/token. Alternatively, you can\n",
            " |              find both your organizations and personal API tokens using\n",
            " |              `HfApi().whoami(token)`.\n",
            " |          gpu (``bool``, `optional`, defaults ``False``):\n",
            " |              Whether to use GPU instead of CPU for inference(requires Startup\n",
            " |              plan at least).\n",
            " |      .. note::\n",
            " |          Setting `token` is required when you want to use a private model.\n",
            " |  \n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def infer(prompt,\n",
        "          max_length = 128,\n",
        "          top_k = 0,\n",
        "          num_beams = 0,\n",
        "          no_repeat_ngram_size = 2,\n",
        "          top_p = 0.9,\n",
        "          seed=42,\n",
        "          temperature=0.7,\n",
        "          greedy_decoding = False,\n",
        "          return_full_text = False):\n",
        "    \n",
        "\n",
        "    top_k = None if top_k == 0 else top_k\n",
        "    do_sample = False if num_beams > 0 else not greedy_decoding\n",
        "    num_beams = None if (greedy_decoding or num_beams == 0) else num_beams\n",
        "    no_repeat_ngram_size = None if num_beams is None else no_repeat_ngram_size\n",
        "    top_p = None if num_beams else top_p\n",
        "    early_stopping = None if num_beams is None else num_beams > 0\n",
        "\n",
        "    params = {\n",
        "        \"max_new_tokens\": max_length,\n",
        "        \"top_k\": top_k,\n",
        "        \"top_p\": top_p,\n",
        "        \"temperature\": temperature,\n",
        "        \"do_sample\": do_sample,\n",
        "        \"seed\": seed,\n",
        "        \"early_stopping\":early_stopping,\n",
        "        \"no_repeat_ngram_size\":no_repeat_ngram_size,\n",
        "        \"num_beams\":num_beams,\n",
        "        \"return_full_text\":return_full_text\n",
        "    }\n",
        "    \n",
        "    s = time.time()\n",
        "    response = inference(prompt, params=params)\n",
        "    #print(response)\n",
        "    proc_time = time.time()-s\n",
        "    #print(f\"Processing time was {proc_time} seconds\")\n",
        "    return response"
      ],
      "metadata": {
        "id": "yxaARTDwxeSi"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Basic Prompts"
      ],
      "metadata": {
        "id": "PbHsGkQw4Adc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The thing that makes large language models interesting is\"\n",
        "resp = infer(prompt)\n",
        "\n",
        "resp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFBENW17xlQN",
        "outputId": "67592947-c65f-4b9a-96ad-c419d89736ec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'The thing that makes large language models interesting is that they are huge. They have so many parameters that the cost of training is used to check whether a given property is defined. The property can be given as an operator in the factor graph of the original problem. This leads to the following equivalence.\\nTheorem 2. The local, el tradicional, el de la cuina'}]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \" #Use OpenCV in Python\"\n",
        "resp = infer(prompt)\n",
        "\n",
        "resp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiSTrZxCxpOC",
        "outputId": "a58e3de1-f526-4a1b-8057-bee157606dbe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': ' #Use OpenCV in Python for face recognition (a bit tricky) #OpenCV and Python (a bit tricky) #Face recognition using Python and OpenCV (a bit tricky) ########\\nthe house, and brought in the news that Mr. Tuke is a character that this is a new kind of paper, but it is not'}]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Advanced Prompting (Predifined format of input, forcing the language model to think in a particular way)"
      ],
      "metadata": {
        "id": "qWKE8JAp3yxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\" **ChatBot**\n",
        "Person: What should I do today? What are my daily tasks?\n",
        "Bot:\"\"\"\n",
        "resp = infer(prompt)\n",
        "\n",
        "print(resp[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkYQFqhW0P0Q",
        "outputId": "8fc9811f-c9bd-46a6-c6f5-9aac5bac0158"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " **ChatBot**\n",
            "Person: What should I do today? What are my daily tasks?\n",
            "Bot: Hi, my name is *BotName*, I am your personal assistant. You can ask me anything and I will help you. What should I do today?\n",
            "Person: I need to buy some food for my family.\n",
            "Bot: I see, I will help you with that. You can ask me anything about food\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp =\"\"\" **Review: **\n",
        "I was apprehensive about ordering a recliner online, so I did extensive research about it, and found that the best deal was available on amazon and the recliner was also comfortable.\n",
        "\n",
        "\n",
        "**Questions: **\n",
        "1. What product is this a review of?\n",
        "2. Is this review overall positive or negative?\n",
        "3. What are the main points?\n",
        "\n",
        "**Answers: **\n",
        "1. \"\"\"\n",
        "\n",
        "resp = infer(inp)\n",
        "\n",
        "print(resp[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOa_Gsd02Duj",
        "outputId": "04b128a7-2a56-4275-9b15-f84bbe3ef029"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " **Review: **\n",
            "I was apprehensive about ordering a recliner online, so I did extensive research about it, and found that the best deal was available on amazon and the recliner was also comfortable.\n",
            "\n",
            "\n",
            "**Questions: **\n",
            "1. What product is this a review of?\n",
            "2. Is this review overall positive or negative?\n",
            "3. What are the main points?\n",
            "\n",
            "**Answers: **\n",
            "1.  This is a review of a recliner chair.\n",
            "2. The review is overall positive. The reviewer says the chair is comfortable, and the price is fair.\n",
            "3. The review says that the chair is comfortable, and that the price is fair.\n",
            "\n",
            "\n",
            "**Notes: **\n",
            "* This is a review question.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp =\"\"\" **Python Code**\n",
        "Write code to add two numbers in python. \"\"\"\n",
        "\n",
        "resp = infer(inp)\n",
        "\n",
        "print(resp[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLa0hjCr3bl2",
        "outputId": "6b28b140-301e-49e7-c21a-6644d889447d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " **Python Code**\n",
            "Write code to add two numbers in python. \n",
            "\n",
            "    \"\"\"\n",
            "def main():\n",
            "    x = int(input(\"Enter a number:\"))\n",
            "    y = int(input(\"Enter another number:\"))\n",
            "    z = x + y\n",
            "    print(\"The sum of \",x,\" and \",y,\" is \",z)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp =\"\"\" **Sample Essay**\n",
        "*Best things in life are free*\n",
        "Nature has a wide variety of things to offer. \"\"\"\n",
        "\n",
        "resp = infer(inp, max_length = 128)\n",
        "\n",
        "print(resp[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXYKnY0t458B",
        "outputId": "27e96229-0c25-4d1d-a356-2d8bcbda6339"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " **Sample Essay**\n",
            "*Best things in life are free*\n",
            "Nature has a wide variety of things to offer.  It is a gift from God. It is the best gift we have ever received. It is a gift that will never run out. Nature has its own beauty and wonder that is hard to describe. Nature is not only beautiful but it is also very important to our lives. I believe that nature is one of the best things in life. I think that nature is the best thing in life because it helps us to have a better life. Nature gives us fresh air, clean water and a beautiful environment to live in. I think that nature is the best thing in life because it makes us happy and healthy. It makes us healthy because it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp =\"\"\" **Python Code**\n",
        "Code to find nth fibonacci number in python.\"\"\"\n",
        "\n",
        "resp = infer(inp)\n",
        "\n",
        "print(resp[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liINtXRF5MhH",
        "outputId": "d00888d6-e4f9-441c-9203-a8633095dd5a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " **Python Code**\n",
            "Code to find nth fibonacci number in python. The code has been tested and is working fine. I have used a recursive function to find nth fibonacci number. The idea of the code is to keep on adding the previous two numbers in the sequence till the nth number is obtained. The code is as follows:\n",
            "def Fibonacci(n):\n",
            "    if n == 0:\n",
            "        return 0\n",
            "    elif n == 1:\n",
            "        return 1\n",
            "    else:\n",
            "        return Fibonacci(n-2) + Fibonacci(n-1)\n",
            "\n",
            "print(Fibonacci(10))Introduction\n",
            "The primary role of the health care system is to provide high quality health care to patients. A key component in providing high quality\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below prompt is from a text of Allan Johnson - \"The Forests and the Trees\". Lets see how well the model understands the underlying information."
      ],
      "metadata": {
        "id": "_fDvZ5BX-NRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inp =\"\"\" **Sample Essay**\n",
        "*Sociology*\n",
        "We should understand how we are participating in the social syatem and not have an individualistic perspective.\"\"\"\n",
        "\n",
        "resp = infer(inp, max_length = 128)\n",
        "\n",
        "print(resp[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xu95wvfv5-wL",
        "outputId": "aef54f36-3619-4578-d19b-304ec979e84a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " **Sample Essay**\n",
            "*Sociology*\n",
            "We should understand how we are participating in the social syatem and not have an individualistic perspective. Individualistic perspectives have many negative consequences such as social isolation, lack of empathy, and loss of the community. Society has many problems because of individualism. We need to have a perspective that takes into account the community. Sociology helps us to understand the problems of society and the community. It helps us understand that we are not alone in the world. We need to understand that the St. Petersburg and Moscow metro stations are the largest in the world, but did you know that the Moscow metro is the deepest and longest in the world? In total, there are 12 lines and more than 200 stations, but the number of stations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SiPUXU558ecM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My Take: There is still a long way to go, but large language models are giving promising results :)"
      ],
      "metadata": {
        "id": "Xz6QqxtUBAU7"
      }
    }
  ]
}